{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "from jaxtyping import Float, Int\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from transformers import PreTrainedModel, AutoModelForCausalLM, AutoTokenizer, StoppingCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ref = \"distilgpt2\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_ref)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-compute which vocabulary IDs correspond to full words\n",
    "full_word_re = re.compile(r\"^[ĠĊ]|[^\\w]\")\n",
    "# There are some punct tokens that are not full words -- contractions. TODO figure out how the tokenizer deals with this. For now we'll have a manual list of exceptions\n",
    "subword_exceptions = [\"'s\", \"'ve\", \"'re\", \"'m\", \"'ll\", \"'d\", \"'t\"]\n",
    "# subword_exception_ids = [tokenizer.encode(subword)[0] for subword in subword_exceptions]\n",
    "\n",
    "full_word_mask = torch.zeros(tokenizer.vocab_size, dtype=torch.bool)\n",
    "for token, idx in tokenizer.get_vocab().items():\n",
    "    full_word_mask[idx] = (idx == tokenizer.eos_token_id or bool(full_word_re.match(token))) and (token not in subword_exceptions)\n",
    "\n",
    "class FullWordStoppingCriteria(StoppingCriteria):\n",
    "    def __call__(self,\n",
    "                 input_ids: Int[Tensor, \"batch seq_len\"],\n",
    "                 scores: Float[Tensor, \"batch vocab_size\"]) -> bool:\n",
    "        \"\"\"\n",
    "        Stop when the last generated token is a full word.\n",
    "        \"\"\"\n",
    "        return bool(full_word_mask[input_ids[-1]].all().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Step 1, fringe size: 100\n",
      "0.0\n",
      "Step 2, fringe size: 100\n",
      "0.4680291712284088\n",
      "Step 3, fringe size: 40\n",
      "Empty fringe. This ain't good but we gotta stop. Try increasing K.\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"Can we please\", return_tensors=\"pt\")\n",
    "top_p = 0.95\n",
    "cum_p = 0.0\n",
    "fringe_size = 100\n",
    "\n",
    "results, result_scores = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs[\"input_ids\"])\n",
    "    next_token_logits = outputs.logits[:, -1, :]\n",
    "    next_token_logprobs = next_token_logits.log_softmax(dim=-1)\n",
    "\n",
    "    # Initialize fringe with top k tokens concatenated to context\n",
    "    indices = torch.topk(next_token_logits, k=fringe_size, dim=-1).indices.T\n",
    "    # fringe: collection of token sequences to be expanded\n",
    "    fringe = torch.cat([inputs[\"input_ids\"].repeat(fringe_size, 1), indices], dim=-1)\n",
    "    # fringe_logprobs: accumulated log-probability within each fringe element\n",
    "    fringe_logprobs = next_token_logprobs[0, indices.squeeze()]\n",
    "\n",
    "    j = 0\n",
    "    while cum_p < top_p:\n",
    "        print(cum_p)\n",
    "        if j > 2:\n",
    "            break\n",
    "        j += 1\n",
    "\n",
    "        print(f\"Step {j}, fringe size: {len(fringe)}\")\n",
    "\n",
    "        if j > 1:\n",
    "            # Stage/pop fringe elements which show new full words\n",
    "            to_stage = full_word_mask[fringe[:, -1]]\n",
    "\n",
    "            # sequences_to_stage = fringe[to_stage, :-1].tolist()\n",
    "            # DEV: track the last token too\n",
    "            sequences_to_stage = fringe[to_stage].tolist()\n",
    "\n",
    "            results.extend(sequences_to_stage)\n",
    "            result_scores.extend(fringe_logprobs[to_stage].tolist())\n",
    "\n",
    "            # Update cumulative probability\n",
    "            cum_p += fringe_logprobs[to_stage].exp().sum().item()\n",
    "\n",
    "            # Remove fringe elements which show new full words\n",
    "            fringe = fringe[~to_stage]\n",
    "            fringe_logprobs = fringe_logprobs[~to_stage]\n",
    "\n",
    "        if len(fringe) == 0:\n",
    "            print(\"Empty fringe. This ain't good but we gotta stop. Try increasing K.\")\n",
    "            break\n",
    "\n",
    "        # Repopulate fringe by expanding the remaining sequences\n",
    "        # Draw `repop_k`-many max-probability continuation tokens for each sequence in fringe\n",
    "        repop_k = 20\n",
    "        # repop_outputs.sequences will be len(fringe) * repop_k\n",
    "        repop_outputs = model.generate(fringe, return_dict_in_generate=True, output_scores=True,\n",
    "                                       pad_token_id=tokenizer.eos_token_id,\n",
    "                                       max_length=fringe.shape[-1] + 1,  # just one more token\n",
    "                                       num_beams=repop_k, num_return_sequences=repop_k)\n",
    "        assert repop_outputs.scores[0].shape == (len(fringe) * repop_k, model.config.vocab_size)\n",
    "\n",
    "        # Draw `fringe_size` of the continuations with greatest total log probability\n",
    "        new_fringe_size = min(fringe_size, len(repop_outputs.sequences))\n",
    "        _, repop_indices = torch.topk(repop_outputs.sequences_scores, k=new_fringe_size, dim=-1)\n",
    "        repop_tokens = repop_outputs.sequences[repop_indices, -1]\n",
    "        repop_logprobs = repop_outputs.sequences_scores[repop_indices]\n",
    "\n",
    "        # Locate originating sequences for topk items\n",
    "        originating_sequences_ = torch.arange(len(fringe)).unsqueeze(-1).repeat(1, repop_k).flatten()\n",
    "        originating_sequences = originating_sequences_[repop_indices]\n",
    "                                   \n",
    "        fringe = torch.cat([fringe[originating_sequences], repop_tokens.unsqueeze(-1)], dim=-1)\n",
    "        # TODO shouldn't add this yet. it may be the logprob of a sentinel. we want to use the sentinel content but not incorporate its logprob\n",
    "        fringe_logprobs = fringe_logprobs[originating_sequences] + repop_logprobs\n",
    "\n",
    "# TODO merge dupes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for seq, prob in zip(fringe, fringe_probs):\n",
    "#     print(tokenizer.convert_ids_to_tokens(seq), prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can', 'Ġwe', 'Ġplease', 'Ċ', 'Ċ'] -5.343081951141357\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġrefrain', 'Ġfrom'] -5.750398635864258\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġspread', 'Ġthe'] -5.864449977874756\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġreach', 'Ġout'] -5.881028652191162\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġnote', 'Ġthat'] -4.9881744384765625\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġcontinue', 'Ġto'] -5.307211875915527\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġrefer', 'Ġto'] -6.079874038696289\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġthank', 'Ġyou'] -6.370788097381592\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdo', 'Ġnot'] -3.342820167541504\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġif', 'Ġyou'] -6.364151477813721\n",
      "['Can', 'Ġwe', 'Ġplease', '.\"', 'Ċ'] -6.490191459655762\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġenable', 'ĠJavaScript'] -4.78279447555542\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġensure', 'Ġthat'] -6.545234203338623\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġunderstand', 'Ġthat'] -6.279345989227295\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġinform', 'Ġyou'] -6.3013014793396\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġreport', 'Ġany'] -5.704664707183838\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġwrite', 'Ġto'] -5.714894771575928\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġfeel', 'Ġfree'] -5.172135353088379\n",
      "['Can', 'Ġwe', 'Ġplease', '?', 'Ċ'] -5.040914535522461\n",
      "['Can', 'Ġwe', 'Ġplease', '!', 'Ċ'] -4.7242536544799805\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġencourage', 'Ġyou'] -6.4229230880737305\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġat', 'Ġleast'] -5.7483110427856445\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġreturn', 'Ġto'] -6.381542205810547\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġlet', 'Ġyou'] -4.071826457977295\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġremember', 'Ġthat'] -5.829769134521484\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġleave', 'Ġa'] -4.646186351776123\n",
      "['Can', 'Ġwe', 'Ġplease', ':', 'Ċ'] -5.873083114624023\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġcheck', 'Ġout'] -6.339962959289551\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġtry', 'Ġto'] -6.194549560546875\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġuse', 'Ġthe'] -4.628360748291016\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġmake', 'Ġsure'] -4.095040798187256\n",
      "['Can', 'Ġwe', 'Ġplease', '.', 'Ċ'] -3.266313076019287\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġremove', 'Ġthe'] -5.9649200439453125\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġjoin', 'Ġthe'] -5.403927326202393\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġenable', 'ĠJavascript'] -4.894000053405762\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġgo', 'Ġto'] -5.4092631340026855\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġlook', 'Ġat'] -5.882380485534668\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġtell', 'Ġyou'] -5.346867084503174\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġhave', 'Ġa'] -4.903712272644043\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġknow', 'Ġthat'] -6.4989728927612305\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġread', 'Ġthe'] -6.087808609008789\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġpass', 'Ġon'] -6.486482620239258\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġfor', 'Ġthe'] -6.080834865570068\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġupdate', 'Ġyou'] -6.06194543838501\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġfind', 'Ġout'] -5.367950439453125\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġturn', 'Ġon'] -5.983375549316406\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġlook', 'Ġforward'] -5.9195356369018555\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġyou', 'Ġto'] -5.459559440612793\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġeveryone', 'Ġwho'] -6.510039806365967\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġsubmit', 'Ġa'] -6.210611343383789\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġask', 'Ġfor'] -5.542514324188232\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġcome', 'Ġback'] -5.840113162994385\n",
      "['Can', 'Ġwe', 'Ġplease', '...', 'Ċ'] -6.1118950843811035\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġcall', 'Ġthe'] -5.6481828689575195\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġshare', 'Ġthis'] -4.166043281555176\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġrespect', 'Ġthe'] -6.02070426940918\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġfollow', 'Ġthe'] -5.38407039642334\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġvisit', 'Ġthe'] -6.259120464324951\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġcontact', 'Ġthe'] -4.85862922668457\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdonate', 'Ġto'] -6.177499294281006\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġcome', 'Ġto'] -5.865713119506836\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġsupport', 'Ġthe'] -5.598888874053955\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġtake', 'Ġa'] -4.720857620239258\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġreview', 'Ġthe'] -6.2849273681640625\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġrefer', 'Ġyou'] -6.336393356323242\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġwait', 'Ġfor'] -6.444004058837891\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġsubmit', 'Ġyour'] -6.249988079071045\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġin', 'Ġthe'] -5.522893905639648\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġvisit', 'Ġour'] -6.282306671142578\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġleave', 'Ġyour'] -4.789368629455566\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġjoin', 'Ġin'] -5.507096290588379\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġaccept', 'Ġyour'] -5.886500835418701\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġlook', 'Ġfor'] -5.973372459411621\n",
      "['Can', 'Ġwe', 'Ġplease', 'âĢ¦', 'Ċ'] -6.709107398986816\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġassist', 'Ġin'] -6.749706745147705\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġthat', 'Ġyou'] -6.088396072387695\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġfind', 'Ġa'] -5.439271450042725\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġassist', 'Ġyou'] -6.755888938903809\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġtry', 'Ġand'] -6.3261799812316895\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġinclude', 'Ġthe'] -5.939947128295898\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġknow', 'Ġwhat'] -6.594918727874756\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġcome', 'Ġforward'] -5.907141208648682\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġallow', 'Ġyou'] -4.835229396820068\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdrop', 'Ġa'] -6.722344398498535\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġwait', 'Ġuntil'] -6.4824628829956055\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġask', 'Ġthat'] -5.619068145751953\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġgive', 'Ġyou'] -4.76715087890625\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġprovide', 'Ġa'] -4.93267297744751\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġthat', 'Ġwe'] -6.113839626312256\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġshare', 'Ġyour'] -4.234514236450195\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġpass', 'Ġthe'] -6.598346710205078\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġsend', 'Ġa'] -4.488649845123291\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġturn', 'Ġoff'] -6.080262660980225\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġtell', 'Ġthe'] -5.471391201019287\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġupdate', 'Ġyour'] -6.175568580627441\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġsay', 'Ġthat'] -6.616515636444092\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġcontact', 'Ġyou'] -4.933945655822754\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġread', 'Ġour'] -6.216189861297607\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġwe'] -6.843024253845215\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġhave'] -5.0180511474609375\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġlet'] -5.039969444274902\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġplease'] -6.955784320831299\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'ĠI'] -6.9854230880737305\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġuse'] -5.115897178649902\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġallow'] -5.131019592285156\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġthe'] -7.056188583374023\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġyou'] -7.057761192321777\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġbe'] -5.1491851806640625\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġget'] -5.15842342376709\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġdo'] -5.1664557456970215\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġgive'] -5.175433158874512\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', ')'] -7.085228443145752\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġtake'] -5.191627502441406\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġforget'] -5.198113918304443\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġnot'] -7.107304096221924\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġmake'] -5.210587978363037\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġask'] -5.210712432861328\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġdo'] -7.1330976486206055\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', ','] -7.148280143737793\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġsend'] -5.248930931091309\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġwant'] -5.249190330505371\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġthank'] -7.159307479858398\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġyour'] -7.161176681518555\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġwill'] -7.164631366729736\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġif'] -7.165836811065674\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġhopefully'] -7.171823501586914\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġgo'] -5.267237663269043\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġall'] -7.181512832641602\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġour'] -7.186542987823486\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġin'] -7.18803071975708\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġalso'] -7.193508625030518\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġjust'] -5.291612148284912\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġneed'] -5.301731109619141\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġtry'] -5.3099822998046875\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġat'] -7.221458911895752\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġ(', 'and', 'Ġothers'] -7.221613883972168\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġwait'] -5.321009635925293\n",
      "['Can', 'Ġwe', 'Ġplease', 'Ġdon', \"'t\", 'Ġhesitate'] -5.322700500488281\n"
     ]
    }
   ],
   "source": [
    "for seq, prob in zip(results, result_scores):\n",
    "    print(tokenizer.convert_ids_to_tokens(seq), prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, return_dict_in_generate=True, output_scores=True,\n",
    "                         num_beams=5, num_return_sequences=5, stopping_criteria=[FullWordStoppingCriteria()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BeamSearchDecoderOnlyOutput(sequences=tensor([[15496,    11,   616,  3290,   318, 13779,    11,   290,   314,  1842,\n",
       "           683,    13,   314,  1842,   683,    13,   314,  1842,   683,    13],\n",
       "        [15496,    11,   616,  3290,   318, 13779,    11,   290,   314,  1842,\n",
       "           683,    13,   314,  1842,   683,    11,   290,   314,  1842,   683],\n",
       "        [15496,    11,   616,  3290,   318, 13779,    11,   290,   314,  1842,\n",
       "           284,   711,   351,   340,    13,   314,  1842,   284,   711,   351],\n",
       "        [15496,    11,   616,  3290,   318, 13779,    11,   290,   314,  1842,\n",
       "           284,   711,   351,   683,    13,   314,  1842,   284,   711,   351],\n",
       "        [15496,    11,   616,  3290,   318, 13779,    11,   290,   314,  1842,\n",
       "           683,    13,   314,  1842,   683,    13,   314,  1842,   683,    11]]), sequences_scores=tensor([-0.8121, -0.8173, -0.8743, -0.8833, -0.9129]), scores=(tensor([[ -3.0483,  -6.7191, -10.8251,  ..., -20.6052, -17.7893,  -7.3971],\n",
       "        [ -3.0483,  -6.7191, -10.8251,  ..., -20.6052, -17.7893,  -7.3971],\n",
       "        [ -3.0483,  -6.7191, -10.8251,  ..., -20.6052, -17.7893,  -7.3971],\n",
       "        [ -3.0483,  -6.7191, -10.8251,  ..., -20.6052, -17.7893,  -7.3971],\n",
       "        [ -3.0483,  -6.7191, -10.8251,  ..., -20.6052, -17.7893,  -7.3971]]), tensor([[-10.3897,  -9.6271, -11.7798,  ..., -18.5516, -14.4975,  -8.8463],\n",
       "        [-10.0806, -10.3353, -13.6257,  ..., -19.8839, -14.2261, -11.6651],\n",
       "        [-10.2840,  -8.3068, -10.2546,  ..., -19.8974, -17.3980,  -3.7277],\n",
       "        [ -7.4978,  -6.6981,  -8.9269,  ..., -19.2389, -16.2295,  -3.1327],\n",
       "        [ -9.5839, -10.6516, -13.6744,  ..., -19.1836, -16.1054, -11.9007]]), tensor([[-10.5259, -10.4374, -14.1676,  ..., -19.1460, -15.1630, -10.3121],\n",
       "        [-11.9001, -11.0133, -15.9704,  ..., -22.7308, -12.3015, -12.9095],\n",
       "        [-11.9349, -11.2111, -15.6431,  ..., -22.8025, -13.0982, -13.3942],\n",
       "        [-12.0451, -11.6384, -16.6870,  ..., -23.9212, -13.6223, -13.5490],\n",
       "        [-10.0306, -10.4716, -13.9786,  ..., -18.7628, -16.5155, -11.5410]]), tensor([[-11.8702, -11.4346, -16.5219,  ..., -23.5301, -13.2545, -13.0214],\n",
       "        [ -9.6769, -11.3721, -15.9737,  ..., -19.6705, -17.1290, -11.8720],\n",
       "        [-12.1398, -11.4108, -16.8058,  ..., -23.2809, -14.5736, -13.4989],\n",
       "        [ -9.1736, -10.7900, -15.3310,  ..., -18.7199, -16.8212, -11.1541],\n",
       "        [-10.1506, -10.3067, -15.0536,  ..., -22.7936, -12.7523, -11.5399]]), tensor([[ -9.5812, -10.8784, -15.5383,  ..., -19.1949, -16.9329, -11.4044],\n",
       "        [-10.9383, -11.7138, -14.4970,  ..., -20.9120, -17.0730, -12.5263],\n",
       "        [ -9.7700, -11.3174, -14.4122,  ..., -20.6532, -16.7728, -11.9114],\n",
       "        [ -9.5578, -11.3102, -15.9895,  ..., -17.6299, -12.3730, -11.3746],\n",
       "        [ -3.0790,  -6.1463, -12.0373,  ..., -21.3623, -18.1643,  -7.6226]]), tensor([[-10.4811, -12.3439, -17.1033,  ..., -23.0213, -18.3100, -13.0533],\n",
       "        [ -3.3228,  -6.9476, -13.3461,  ..., -19.7435, -16.6490,  -8.6957],\n",
       "        [ -3.0602,  -6.4826, -12.7002,  ..., -21.7760, -18.2963,  -7.8945],\n",
       "        [ -2.0791,  -6.5489, -13.0337,  ..., -22.3267, -17.8053,  -7.1579],\n",
       "        [-10.4180,  -8.4862, -10.6325,  ..., -20.6229, -18.6351,  -3.4200]]), tensor([[ -5.2516,  -8.7174, -14.4447,  ..., -18.2288, -18.2273, -10.9770],\n",
       "        [-10.6095,  -8.6582, -10.9601,  ..., -21.4667, -19.7724,  -3.1577],\n",
       "        [-10.2379,  -8.5640, -10.8707,  ..., -21.1688, -19.4557,  -2.7602],\n",
       "        [-10.6133,  -8.7552, -10.9581,  ..., -21.6832, -19.8403,  -3.1474],\n",
       "        [-10.9286, -10.0183, -13.5153,  ..., -19.2531, -16.7747,  -8.0033]]), tensor([[ -8.1448,  -9.3482, -15.6349,  ..., -17.9696, -18.0313, -12.5681],\n",
       "        [-12.2602, -11.4835, -16.6422,  ..., -23.2897, -12.9549, -12.7262],\n",
       "        [-12.1897, -11.5247, -16.3604,  ..., -23.3314, -13.1208, -12.5310],\n",
       "        [-10.5668,  -4.2023,  -7.9083,  ..., -22.3603, -21.8588,  -5.7972],\n",
       "        [-12.1358, -11.4717, -16.3473,  ..., -23.2985, -13.0274, -12.5791]]), tensor([[ -2.8521,  -6.6322, -13.5049,  ..., -21.4739, -18.6827,  -7.9561],\n",
       "        [ -2.9881,  -6.9815, -13.2227,  ..., -21.3133, -18.6916,  -8.2110],\n",
       "        [ -3.2489,  -7.6241, -14.0609,  ..., -18.8007, -17.1753,  -8.8358],\n",
       "        [-10.5935, -11.2268, -16.6806,  ..., -19.6870, -18.0137, -11.9641],\n",
       "        [-11.2428, -10.9835, -16.4908,  ..., -16.3287, -17.0420, -12.3063]]), tensor([[-10.1499,  -8.8320, -11.2441,  ..., -21.3932, -20.2513,  -3.0382],\n",
       "        [ -3.6274,  -7.7703, -14.9691,  ..., -22.0779, -18.9977,  -9.4927],\n",
       "        [-10.4538,  -8.9154, -11.2845,  ..., -21.5666, -20.4207,  -3.2685],\n",
       "        [-10.5340,  -8.9960, -11.2331,  ..., -21.8388, -20.4990,  -3.3170],\n",
       "        [-10.9373, -10.5410, -14.7597,  ..., -18.8518, -17.2644,  -9.2917]]), tensor([[-11.2650,  -9.2398, -11.9725,  ..., -22.3381, -21.0977,  -3.2432],\n",
       "        [-12.3673, -11.7865, -16.5710,  ..., -23.3885, -13.6216, -12.8460],\n",
       "        [-12.4040, -11.7428, -16.7810,  ..., -23.2677, -13.3779, -12.9147],\n",
       "        [-10.4436,  -4.5542,  -8.0815,  ..., -22.6709, -22.3478,  -5.7790],\n",
       "        [-11.0885, -11.8410, -15.7453,  ..., -19.4971, -17.6015,  -9.4501]]), tensor([[-12.5722, -12.2242, -17.5267,  ..., -24.0055, -14.4421, -13.4219],\n",
       "        [-11.7806, -12.2508, -17.0965,  ..., -20.2345, -16.9860, -11.9068],\n",
       "        [ -9.3865, -11.4669, -16.0333,  ..., -18.6857, -16.6288, -11.5228],\n",
       "        [-10.3464,  -4.6046,  -7.8404,  ..., -21.9324, -21.7334,  -5.6646],\n",
       "        [ -9.7880, -11.4491, -16.3634,  ..., -18.9989, -17.1623, -12.0279]]), tensor([[-10.8061, -11.4173, -17.0088,  ..., -19.8682, -18.7936, -12.0469],\n",
       "        [-12.6172, -12.5474, -18.3949,  ..., -24.7093, -15.5721, -13.7796],\n",
       "        [-10.9194, -13.1895, -17.8964,  ..., -22.6436, -18.3355, -13.7322],\n",
       "        [-11.1882, -12.9867, -18.0544,  ..., -22.2583, -18.1439, -13.7655],\n",
       "        [-10.5863, -10.7066, -15.9821,  ..., -24.0100, -13.9267, -11.5000]]), tensor([[ -3.8848,  -8.3234, -14.9837,  ..., -21.9066, -19.0678,  -9.1511],\n",
       "        [-11.3702, -11.8310, -17.5439,  ..., -20.2678, -19.2239, -12.2488],\n",
       "        [ -7.6587, -10.0002, -15.8804,  ..., -17.9854, -18.5118, -12.3912],\n",
       "        [ -8.7954, -10.3184, -16.3675,  ..., -17.9558, -18.7770, -13.2005],\n",
       "        [-10.4059, -12.0082, -17.7809,  ..., -20.9519, -20.0576, -12.7082]])), beam_indices=tensor([[ 0,  0,  0,  0,  0,  2,  1,  1,  3,  1,  0,  0,  0,  0, -1, -1, -1, -1,\n",
       "         -1, -1],\n",
       "        [ 0,  0,  0,  0,  0,  2,  1,  1,  3,  1,  4,  1,  1,  1, -1, -1, -1, -1,\n",
       "         -1, -1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  2,  2, -1, -1, -1, -1,\n",
       "         -1, -1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  2,  4,  3,  3, -1, -1, -1, -1,\n",
       "         -1, -1],\n",
       "        [ 0,  0,  0,  0,  0,  2,  1,  1,  3,  1,  0,  0,  0,  0, -1, -1, -1, -1,\n",
       "         -1, -1]]), attentions=None, hidden_states=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False,  True,  True, False, False,  True,  True,  True, False, False,\n",
       "         True, False, False, False,  True, False, False, False,  True, False])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_word_mask[outputs.sequences[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my dog is cute, and I love him. I love him. I love him.\n",
      "Hello, my dog is cute, and I love him. I love him, and I love him\n",
      "Hello, my dog is cute, and I love to play with it. I love to play with\n",
      "Hello, my dog is cute, and I love to play with him. I love to play with\n",
      "Hello, my dog is cute, and I love him. I love him. I love him,\n"
     ]
    }
   ],
   "source": [
    "for seq in outputs.sequences:\n",
    "    print(tokenizer.decode(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'Ġmy',\n",
       " 'Ġdog',\n",
       " 'Ġis',\n",
       " 'Ġcute',\n",
       " ',',\n",
       " 'Ġand',\n",
       " 'ĠI',\n",
       " 'Ġlove',\n",
       " 'Ġto',\n",
       " 'Ġplay',\n",
       " 'Ġwith',\n",
       " 'Ġit',\n",
       " '.',\n",
       " 'ĠI',\n",
       " 'Ġlove',\n",
       " 'Ġto',\n",
       " 'Ġplay',\n",
       " 'Ġwith']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(outputs.sequences[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainscore",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5699e56a57c9c27f2b32ab8be5d7742a125e8d837ad750c467a7346f551880d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
